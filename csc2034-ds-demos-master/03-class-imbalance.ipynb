{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMac-SCg9IdC"
   },
   "source": [
    "# Effects of Class Imbalance\n",
    "## CSC2034\n",
    "### Based on Cameron Trotter \n",
    "\n",
    "\n",
    "In previous notebooks, we have worked with balanced synthetic data. In reaality, the data you work with will often be imbalanced, with some classes containing more examples than others. In this practical, we will examine the effect that class imbalance can have on the performance of models you create. \n",
    "\n",
    "### Google Colab Setup\n",
    "\n",
    "All of the notebooks you will be running in these lab sessions are designed to be ran using [Google Colab](https://colab.research.google.com/). For setup instructions, see this repo's README. \n",
    "\n",
    "In order to make things work on colab, we need to clone this repo and then (in another cell because colab dictates this...) move into the repo directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yM_DZNGM9IdF"
   },
   "outputs": [],
   "source": [
    "!git clone https:// #please put the directory to the git hub here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1KfElJr9IdG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('csc2034-ds-demos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8kUS3_-9IdH"
   },
   "source": [
    "### Generate an unbalanced dataset\n",
    "\n",
    "Whilst we can use `sklearn` to create datasets, these will be balanced. Here we want a dsynthetic dataset that is unbalanced. To achieve this, we can manually flip a set percentage of data point classes. First, lets recap making a dataset. \n",
    "\n",
    "Task: Using the notebooks previously create a balanced classification dataset. I have provided hyperparameters for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-Q7g0GR9IdI"
   },
   "outputs": [],
   "source": [
    "#...\n",
    "\n",
    "n_samples = 1000\n",
    "n_classes = 2\n",
    "n_features = 2\n",
    "n_clusters_per_class = 2\n",
    "n_redundant = 0\n",
    "n_informative = 2\n",
    "random_state = 5\n",
    "flip_y = 0.1\n",
    "\n",
    "data, labels = #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xkPuQ8Y9IdJ"
   },
   "source": [
    "Now that we have a dataset, lets see how many examples of each class we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gns5vJxH9IdK",
    "outputId": "ed94f02c-2ac7-4b63-8404-118fb232f766"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import show_scatterplot\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(f\"Number of class 0 examples: {counts[0]}\")\n",
    "print(f\"Number of class 1 examples: {counts[1]}\")\n",
    "\n",
    "show_scatterplot(data, labels, \"Balanced synthetic dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRCrI2Vj9IdL"
   },
   "source": [
    "Run the below checks. If any return False, take another look at the code you have written before continuing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrTQzpzC9IdN",
    "outputId": "eae4428a-e72c-4e49-8f02-ac82568b78e5"
   },
   "outputs": [],
   "source": [
    "print(f\"Class balance check, number of class 0 examples: {counts[0] == 511}\")\n",
    "print(f\"Class balance check, number of class 1 examples: {counts[1] == 489}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXWC3M7P9IdP"
   },
   "source": [
    "As you can see, our data is mostly balanced. There are only 22 more examples of class 0 than class 1. To really create a class imbalance, I have produced some code (found in `helpers.py`) which will skew this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRseFtZI9IdP",
    "outputId": "77ca8043-af9f-4514-d48e-d34c17752700"
   },
   "outputs": [],
   "source": [
    "from helpers import create_imbalance\n",
    "\n",
    "percentage_imbalance = 0.9\n",
    "imbalanced_labels = create_imbalance(labels, percentage_imbalance)\n",
    "\n",
    "unique, counts = np.unique(imbalanced_labels, return_counts=True)\n",
    "print(f\"Number of class 0 examples: {counts[0]}\")\n",
    "print(f\"Number of class 1 examples: {counts[1]}\")\n",
    "\n",
    "show_scatterplot(data, imbalanced_labels, \"Balanced synthetic dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRu1CNwI9IdR"
   },
   "source": [
    "### Dealing with class imbalance\n",
    "\n",
    "\n",
    "Now our dataset is heavily skewed towards class 1 examples, lets take a look at the different ways we can deal with this. \n",
    "\n",
    "#### Approach 1: Downsampling\n",
    "\n",
    "Downsampling is one method for dealing with imbalanced data. With this approach, the majority class is reduced to closely match the minority class by simply removing examples of the majority class from the dataset. Code to achieve this is provided in `helpers.py`. Spend some time reading and understanding this code.\n",
    "\n",
    "For all types of augmentation, it is important we only perform this on the training set. The test set should be left as is to ensure our classifier's evaluation metrics are not biased by any changes made to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpJjYQIp9IdR",
    "outputId": "77f3bbf3-e8b4-4c52-e641-8618ccfa694e"
   },
   "outputs": [],
   "source": [
    "from helpers import downsample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data,\n",
    "                                                                    imbalanced_labels,\n",
    "                                                                    test_size = 0.33,\n",
    "                                                                    random_state = 5)\n",
    "\n",
    "\n",
    "downsampled_data, downsampled_labels = downsample(data_train, labels_train)\n",
    "\n",
    "show_scatterplot(downsampled_data, downsampled_labels, \"Downsampled synthetic training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s5Wmts-9IdT"
   },
   "source": [
    "On the plus side, we now have a more balanced dataset. On the downside, we now have far less data to train a model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPOt3ZCm9IdT"
   },
   "source": [
    "#### Approach 2: SMOTE\n",
    "\n",
    "In some cases, such as the one above, we may not want to downsample our dataset as this would leave us with too few data points to train a model on. What if we could do the opposite - that is, oversample the data to create more datapoints?\n",
    "\n",
    "SMOTE, or Synthetic Minority Over-Sampling Technique, allows you to do just that! Proposed by Chawla *et al.* in 2002, the method generates fake data for the minority class which mirrors the qualities of the real data. This can be very powerful!\n",
    "\n",
    "If you want to learn more about SMOTE, you can read the paper [here](https://www.jair.org/index.php/jair/article/view/10302). SMOTE itself has been implemented as part of the [imbalanced-learn](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html?highlight=smote#imblearn.over_sampling.SMOTE) python package. \n",
    "\n",
    "Task: Using the package documentation linked above, utilise SMOTE to oversample the imbalanced training data we created. Ensure the `random_state` below is used. It is fine to use default parameters for all other arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACnOdA_b9IdU"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "random_state = 5\n",
    "\n",
    "#...\n",
    "\n",
    "smote_train_data, smote_train_labels = #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaHgRB9i9IdV"
   },
   "source": [
    "SMOTE will attempt to create an equal number of examples for each class. Let's see if that has happened..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0elZKW79IdW",
    "outputId": "42374a58-f124-43aa-b086-b49b7fd47bfe"
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(smote_train_labels, return_counts=True)\n",
    "\n",
    "print(f\"After SMOTE, number of class 0 examples: {counts[0]}\")\n",
    "print(f\"After SMOTE, number of class 1 examples: {counts[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTyUcErM9IdW"
   },
   "source": [
    "Run the below checks. If any return False, take another look at the code you have written before continuing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-MSXGPK9IdX",
    "outputId": "a1c5e6b3-a2c0-4fed-cd10-8413608b93e0"
   },
   "outputs": [],
   "source": [
    "print(f\"SMOTE check, number of class 0 examples: {counts[0] >= 599 and counts[0] <= 615}\")\n",
    "print(f\"SMOTE check, number of class 1 examples: {counts[0] >= 599 and counts[0] <= 615}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZfuVDcF9IdX"
   },
   "source": [
    "Let's plot the distribution of data we have after SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwSU3b5X9IdY",
    "outputId": "2c7c55a0-0f49-466d-8b77-1d51d6dde686"
   },
   "outputs": [],
   "source": [
    "show_scatterplot(smote_train_data, smote_train_labels, \"Synthetic dataset after SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69hAmwlp9IdZ"
   },
   "source": [
    "As you can see, SMOTE has utilised the existing datapoints for the minority class and has used them to create synthetic data that closely mimics them. We can then use data created using SMOTE in the same way as before to train models. \n",
    "\n",
    "Task: Using a model of your choice, train a model on the SMOTE data and generate evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Dc1O8oF9Ida"
   },
   "outputs": [],
   "source": [
    "#..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
